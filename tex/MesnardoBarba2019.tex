\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage{algorithmic}
\usepackage{array}
\usepackage[nocompress]{cite}
\usepackage{color}
\usepackage{listings}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{stfloats}
\usepackage{url}
\usepackage{multirow}

\usepackage{listings}
\lstset{
    backgroundcolor=\color{white},
    basicstyle=\footnotesize,
    breakatwhitespace=true,
    breaklines=true,
    captionpos=b,
    extendedchars=true,
    frame=single,
    keepspaces=true,
    language=bash,
    morekeywords={*,...},
    numbers=none,
    numbersep=5pt,
    rulecolor=\color{black},
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    stepnumber=2,
    tabsize=2,
    title=\lstname,
}

\usepackage[pdftex]{graphicx}
\graphicspath{{./figs}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}


% *** MATH PACKAGES ***
\usepackage{amsmath}
\interdisplaylinepenalty=2500

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

% title
\title{Reproducible Workflow on a Public Cloud for Computational Fluid Dynamics}
% author names and affiliations
\author{Olivier Mesnard, Lorena A. Barba
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Mechanical and Aerospace Engineering,
the George Washington University, Washington, DC 20052.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: mesnardo@gwu.edu
\IEEEcompsocthanksitem Email: labarba@gwu.edu}% <-this % stops an unwanted space
%\thanks{Manuscript submitted 2019}
}

\IEEEtitleabstractindextext{%
\begin{abstract}
In a new effort to make our research transparent and reproducible by others, we have developed a workflow to run computational studies on a public cloud. It uses Docker containers to create an image of the application software stack. We also adopt several tools that facilitate creating and managing virtual machines with compute nodes and submitting jobs to these nodes. The configuration files for these tools are part of an expanded "reproducibility package" that includes workflow definitions for cloud computing, in addition to input files and instructions. This facilitates re-creating the cloud environment to re-un the simulations under the same conditions.
\end{abstract}
}

% make the title area
\maketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\section{Reproducible Workflow}\label{sec:workflow}

CFD publications often lack details about external libraries used along with the main computational code.
We have learned the hard way how different versions of the same external library can alter the numerical results and even the scientific findings of a computational study\cite{mesnard_barba_2017}.
To overcome the so-called ``dependency hell'' and facilitate reproducibility, we use Docker to locally create an image of our computational application.
Container technology facilitates share build images of the full software stack that is used to produce scientific results.
The process of building an image using Docker begins with writing an ASCII file, called a Dockerfile, that contains Docker keywords and system commands for building multi-layered software stack.
Once the image is build locally, we push it to a public registry (e.g., DockerHub).

\begin{lstlisting}
$ cd $CLOUDREPRO/docker/petibm
$ docker build --tag=barbagroup/petibm:0.4-GPU-IntelMPI-ubuntu --file=Dockerfile .
$ docker push barbagroup/petibm:0.4-GPU-IntelMPI-ubuntu
\end{lstlisting}

Everyone can now pull the application image and create a container to run the application in a faithfully reproduced local environment.
We aim to move the final stage of this workflow (creating a container) to a public cloud provider such as Microsoft Azure.

To run computational jobs on Microsoft Azure, we use several tools that facilitate creating and managing virtual machines with compute nodes and submitting jobs to those nodes.
\ref{fig:cloud_workflow} shows a graphical representation of the workflow we adopted to run CFD simulations with an in-house software on Microsoft Azure.
We use a Microsoft Azure platform service called Azure Batch to run our in-house CFD software.
Azure Batch services leverages Microsoft Azure at no extra cost to relieve the user from manually creating, configuring, and managing a HPC-capable cluster of cloud nodes, including virtual machines, virtual networks, job and task scheduling infrastructure.
Azure Batch works with both embarrassingly parallel workloads and tightly-coupled MPI jobs (the latter being the case of our CFD software).
To use Azure Batch, the user needs to configure a workspace on Microsoft Azure by creating an Azure Batch account associated to an Azure Storage account (to upload and download simulation data).
This can be done either via the Azure Portal in a web browser or from a local terminal user the open-source tool Azure CLI.\footnote{Azure CLI (version 2.0.57): \url{https://github.com/Azure/azure-cli}}

\begin{lstlisting}
$ az account set --subscription reprosubscription
$ az group create --name reprorg --location eastus
$ az storage account create --name reprostorage --resource-group reprorg --sku Standard_LRS --location eastus
$ az batch account create --name reprobatch --resource-group reprorg --location eastus --storage-account reprostorage
$ az storage share create --name fileshare --account-name reprostorage --account-key storagekey --quota 100
\end{lstlisting}

To create computational nodes and submit container-based jobs to Azure Batch, we use the open-source command-line utility Batch Shipyard.\footnote{Batch Shipyard (version 3.6.1): https://github.com/Azure/batch-shipyard} Batch Shipyard reads user-written YAML configuration files to automatically create pools of compute nodes on Microsoft Azure and to submit jobs to those pools.

\begin{itemize}
    \item \texttt{config.yaml} contains information about the Azure Storage account and Docker images to use.
    \item \texttt{credentials.yaml} stores the necessary credentials to use the different Microsoft Azure service platforms (e.g., Azure Batch and Azure Storage).
    \item \texttt{pool.yaml} is where the user configures the pool of virtual machines to create.
    \item \texttt{jobs.yaml} details the configuration of the jobs to submit to the pool.
\end{itemize}

The Docker image of our CFD application is pulled from the (public or private) registry to the virtual machines when the pool is created.

\begin{lstlisting}
$ cd $CLOUDREPRO/examples/snake2d2k35
$ az storage directory create --name snake2d2k35 --share-name fileshare --account-name reprostorage
$ export SHIPYARD_CONFIGDIR=config_shipyard
$ shipyard pool add
$ shipyard data ingress
$ shipyard jobs add
\end{lstlisting}

Once the simulations are done (i.e., the job tasks are complete), we delete the jobs and the pool.
The output of the computation is now stored on a fileshare in our Azure Storage account and we can download the data to our local machine to perform additional post-processing steps.

\begin{lstlisting}
$ shipyard pool del
$ mkdir output
$ az storage file download-batch --source fileshare/snake2d2k25 --destination output --account-name reprostorage
\end{lstlisting}

Reproducible research requires authors to make their code and data available.
Thus, the Dockerfile and YAML configuration files should be made part of an extended reproducibility package that includes workflow instructions for cloud computing, in addition to other input files.
Such reproducibility package facilitates re-creating the cloud environment to run the simulations under the same conditions.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=14cm]{figures/cloud_workflow.png}
    \caption{Reproducible workflow on the public cloud provider Microsoft Azure.}
    \label{fig:cloud_workflow}
\end{figure*}

\section{Results}\label{sec:results}

Thanks to a sponsorship ($\$20,000$ worth of cloud credits) from the Microsoft Azure for Research program,\footnote{Microsoft Azure for Research: \url{https://www.microsoft.com/en-us/research/academic-program/microsoft-azure-for-research/}} three-dimensional CFD simulations with our in-house research software ran on the public cloud platform using the reproducible workflow described in Section \ref{sec:workflow}.
Preliminary results from benchmarks and test-cases showed that we are able to obtain similar performance in terms of latency and bandwidth using the Azure virtual network, comparing to a traditional University-managed HPC cluster (Colonial One).
Table \ref{tab:hw_specs} reports the hardware specifications of the nodes used on Microsoft Azure and Colonial One.

\begin{table*}[!h]
    \renewcommand{\arraystretch}{1.5}
    \caption{Hardware Specifications of Nodes Used on Microsoft Azure and Colonial One. On Both Platforms, MPI Applications Take Advantage of RDMA (Remote Direct Memory Access) Network with FDR InfiniBand.}
    \label{tab:hw_specs}
    \centering
    \begin{tabular}{cccccccc}
        Platform & Node & Intel Xeon CPU & \# threads & NVIDIA GPU & RAM (GiB) & SSD Storage (GiB) \\
        \hline
        \multirow{2}{*}{Azure} & \texttt{NC24r} & Dual 12-Core E5-2690v3 (2.60GHz) & 24 & 4 x K80 & 224 & 1440 \\
        & \texttt{H16r} & Dual 8-Core E5-2667v3 (3.20GHz) & 16 & - & 112 & 2000 \\
        \hline
        \multirow{2}{*}{Colonial One} & \texttt{Ivygpu} & Dual 6-Core E5-2620v2 (2.10GHz) & 12 & 2 x K20 & 120 & 93 \\
        & \texttt{Short} & Dual 8-Core E5-2650v2 (2.60GHz) & 16 & - & 120 & 93 \\
        \hline
    \end{tabular}
\end{table*}

\subsection{MPI Communication Benchmarks}\label{subsec:mpi_benchmarks}

Point-to-point MPI benchmarks from the Ohio State University Micro-Benchmarks\footnote{OSU Micro-Benchmarks (version 5.6): \url{http://mvapich.cse.ohio-state.edu/benchmarks/}} suite ran on Microsoft Azure and Colonial One to investigate performance in terms the latency and bandwidth.
The latency test is carried out in a ping-pong fashion and measures the time elapsed to get a response; the sender sends a message with a certain data size and wait for the receiver to send back the message with the same data size.
The bandwidth test measures the maximum sustained rate that can be achieved on the network; it consists in having the sender sends a fixed number of messages to a receiver that replies only after receiving all of them.
The tests ran on \texttt{NC24r} nodes on Azure and \texttt{Ivygpu} nodes on Colonial One, all them featuring a network interface for RDMA (Remote Direct Memory Access) connectivity to communicate over an InfiniBand network.
Fig. \ref{fig:osu_benchmarks} reports the mean latencies and bandwidths obtained over $5$ repetitions on both platforms.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/osu_latency_bandwidth.pdf}
    \caption{Point-to-point latency (top) and bandwidth (bottom) obtained on Colonial One (\texttt{Ivygpu} nodes) and on Microsoft Azure (\texttt{NC24r} nodes). Benchmark results are averaged over $5$ repetitions.}
    \label{fig:osu_benchmarks}
\end{figure}

\subsection{Poisson Benchmarks}\label{subsec:poisson_benchmarks}

CFD algorithms often require to solve linear systems with iterative methods.
For example, the project method implemented in our research software solve a Poisson system every time step to project the velocity field onto the divergence-free space (to satisfy the incompressibility constraint).
Thus, we investigated the time-to-solution to solve a three-dimensional Poisson system (obtained with a 7-point stencil using central differences) on different nodes of Microsoft Azure and Colonial One.
The system was solved using a Conjugate-Gradient (CG) method with a classical Algebraic MultiGrid (AMG) preconditioner with an exit criterion set to an absolute tolerance of $10^{-12}$.
The iterative solver ran on CPU nodes (\texttt{H16r} instances on Azure and \texttt{Short} nodes on Colonial One) using the CG algorithm from the PETSc library and an AMG preconditioner from Hypre BoomerAMG.
Fig. \ref{fig:poisson_benchmarks} (top) reports the mean runtimes (averaged over $5$ repetitions) to iteratively solve the system, on a uniform grid of $50$ million cells ($1000 \times 1000 \times 50$), as we increase the number of nodes in the pool.
We also solved the Poisson system with the NVIDIA AmgX library on multiple GPU devices using \texttt{NC24r} instances on Azure and \texttt{Ivygpu} nodes on Colonial One.
The Poisson system for a base mesh of $6.25$ million cells ($500 \times 500 \times 25$) is solved on a single node using $12$ MPI processes and $2$ GPU devices; we then double the mesh size as we double the number of nodes.
As the number of iterations to reach convergence increases with the size of the system, we normalize the runtimes by the number of iterations.
Fig. \ref{fig:poisson_benchmarks} (bottom left) shows the normalized mean runtimes ($5$ repetitions) obtained on Azure and Colonial One.
The bottom-right panel of the figure reports the normalized mean runtimes obtained on Azure when we put more constraint on the bandwidth; the base mesh now contains $25$ millions cells ($1000 \times 500 \times 50$).

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/poisson_time_vs_nodes.pdf}
    \caption{Runtimes to solve a Poisson system on Colonial One and on Microsoft Azure. Benchmark was repeated 5 times and we report the mean runtimes as well as the extrema. Poisson system was solved on Azure using PETSc on \texttt{H16r} nodes and using AmgX on \texttt{NC24r} nodes. On Colonial One, we solved the system with PETSc on \texttt{Short} nodes and with AmgX on \texttt{Ivygpu} nodes. The runs with PETSc solved a Poisson system on a mesh size of 50 million cells (top). With AmgX (bottom left), the base mesh size contains 6.25 million cells and we scale it with the number of nodes. On Azure, we also ran the Poisson benchmark with a finer base mesh size of 25 million cells (bottom right). Runtimes obtained with AmgX were normalized by the number of iterations required to reach a absolute tolerance of $10^{-12}$.}
    \label{fig:poisson_benchmarks}
\end{figure}

\subsection{Flow Around a Flying Snake Cross-Section}\label{subsec:snake}

Our research lab is interested in understanding the aerodynamics of flying animals using Computational Fluid Dynamics software.
One of our applications deals with the aerodynamics of a snake species, \textit{Chrysopelea Paradisi}, that lives in South-East Asia.
The arboreal reptile has the remarkable capacity to turn its entire body into a wing and glide over several meters\cite{socha_2011}.
The so-called flying snake jumps from tree branches, undulates in the air, and is able to produce lift by expanding its ribcage to flatten its ventral surface (morphing its normally circular cross-section into a triangular shape).

To study the flow around the flying snake, we develop a in-house CFD software called PetIBM\cite{chuang_et_al_2018}, a open-source toolbox that solves the two- and three-dimensional incompressible Navier-Stokes equations using a projection method (an approximate block-LU decomposition of the fully discretized equations\cite{perot_1993}) and an Immersed-Boundary Method (IBM).
Within this framework, the fluid equations are solved on a extended grid that does not conform to the surface of the immersed body.
To model the presence of the body, the momentum equation is augmented with a forcing term that is activated in the vicinity of the immersed boundary.
This technique allows the use of simple fixed structured Cartesian grids to solve the equations.
PetIBM also implements Immersed Boundary Methods (IBMs) that fit into the projection method and, in the present study, we use an implementation of the IBM proposed in \cite{li_et_al_2016}.
To obtain reproducible computational results, the code used to run the simulations needs to be made available.
In that regard, PetIBM is open source, version-controlled on GitHub,\footnote{PetIBM (version 0.4): \url{https://github.com/barbagroup/PetIBM}} and developed under the permissive BSD-3 clause license.

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/wz_multi_contourf.png}
    \caption{Filled contour of the vorticity field ($-5 \leq w_z c / U_\infty \leq 5$) after $20$, $44$, $45$, and $80$ time units of flow simulation with PetIBM for the snake cross-section at a $35$-degree angle of attack and Reynolds number $2000$. Vortex merging events are responsible for the change in the wake signature and the drop in the mean value of the lift coefficient.}
    \label{fig:wz_2d}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/forceCoefficientsCompare2D.pdf}
    \caption{History of the force coefficients obtained with two- and three-dimensional simulations at Reynolds number $2000$ for a snake cross-section with a $35$-degree angle of attack.}
    \label{fig:force_coefficients}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=8cm]{figures/wz_avg_multi_contourf.png}
    \caption{Filled contour of the spanwise-averaged z-component of the vorticity ($-5 \leq w_z c / U_\infty \leq 5$) field after $80$ and $100$ time units of flow simulation with PetIBM for the snake cylinder with a cross-section at a $35$-degree angle of attack and Reynolds number $2000$.}
    \label{fig:wz_avg_3d}
\end{figure}

\begin{table}[!h]
    \caption{Time-averaged force coefficients on the snake cross-section at Reynolds number 1000 and angle of attack $35^o$ for the two- and three-dimensional configurations. (We average the force coefficients between $40$ and $80$ time units of flow simulation.)}
    \label{tab:force_coefficients}
    \centering
    \begin{tabular}{ccll}
        \hline
        Case & Mesh size & $<C_D>$ & $<C_L>$ \\
        \hline
        3D & $1071 \times 1072 \times 40$ & $0.8390$ & $1.5972$ \\
        2D & $1704 \times 1706$ & $1.1567$ ($+37.9\%$) & $1.8279$ ($+14.4\%$) \\
        \hline
    \end{tabular}
\end{table}

\section{Cost Analysis and Lessons Learned}\label{sec:cost}

% Mention the 7-day limitation for jobs with Azure Batch.
% Mention that Batch Shipyard can pull images from public and private registries.
% Mention that Singularity containers are also supported in Batch Shipyard.

\begin{table*}[!h]
    \caption{NC series on Microsoft Azure. (Prices as of May 13, 2018.)}
    \label{tab:nc_series}
    \centering
    \begin{tabular}{cccccccc}
        Instance & cores & RAM & disk sizes & GPU & pay-as-you-go & 1-year reserved & 3-year reserved \\
        && (GiB) & (Gib) && (\$/hr) & (\$/hr) & (\$/hr) \\
        \hline
        NC6 & 6 & 56 & 340 & 1 x K80 & 0.90 & 0.574 & 0.40 \\
        NC12 & 12 & 112 & 680 & 2 x K80 & 1.80 & 1.147 & 0.80 \\
        NC24 & 24 & 224 & 1,440 & 4 x K80 & 3.60 & 2.294 & 1.599 \\
        NC24r\footnotemark & 24 & 224 & 1,440 & 4 x K80 & 3.96 & 2.523 & 1.758 \\
        \hline
    \end{tabular}
\end{table*}
\footnotetext{NC24r instances are RDMA (Remote Direct Access Memory) capable with InfiniBand network.}

% \section*{Acknowledgment}

\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{IEEEabrv,MesnardoBarba2019}
%
% <OR> manually copy in the resultant .bbl file
%\begin{thebibliography}{1}
%\bibitem
%\end{thebibliography}

\end{document}


